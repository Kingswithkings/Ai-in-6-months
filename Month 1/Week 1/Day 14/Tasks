# Imports
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

print('PyTorch version:', torch.__version__)


# Task 1 : Creation and OPerations (Tensors are the core data structure in PyTorch, similar to NumPy arrays but with GPU support and autograd).

# Create tensors
X = torch.tensor([1.0, 2.0, 3.0])
print('x:', X)

# From numpy
arr = np.array([4.0, 5.0, 6.0])
t = torch.from_numpy(arr)
print('from numpy:', t)

# Random tensors
r = torch.randn(2,3)
print('random:', r)

# Basic operations
print('x + x', X + X)
print('x * 2:', X * 2)


# Task 2 Autograd - gradients (Use requires_grad=True to track operations for automatic differentiation.)
# Autograd example
a = torch.tensor(2.0, requires_grad=True)
b = torch.tensor(3.0, requires_grad=True)

c = a * b + a**2
print('c:', c)

# Compute gradients
c.backward()
print('dc/da (grad of a ):', a.grad)
print('dc/db (grad of b):', b.grad)


# Task 3 Build a Simple MLP (classification) and train on synthetic data
# We'll build a 2-layer MLP to classify points from two Gaussian blobs.

# Create synthetic dataset (2D) - two blobs
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=500, centers=2, random_state=42)
X = X.astype(np.float32)
y = y.astype(np.int64)

# Convert to torch tensors
tX = torch.from_numpy(X)
ty = torch.from_numpy(y)

# Visualise data
import matplotlib.pyplot as plt
plt.figure(figsize=(6,4))
plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', s=10)
plt.title('Synthetic blobs')
plt.show()

# Define Model: simple 2-layer MLP
class SimpleMLP(nn.Module):
    def __init__(self, in_dim=2, hidden=16, out_dim=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, out_dim)
        )
    def forward(self, X):
        return self.net(X)
        
model = SimpleMLP()
print(model)

# Training setup
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Train/val split
num_samples = tX.shape[0]
idx = torch.randperm(num_samples)
train_idx = idx[:int(0.8*num_samples)]
val_idx = idx[int(0.8*num_samples):]

X_train = tX[train_idx]
y_train = ty[train_idx]
X_val = tX[val_idx]
y_val = ty[val_idx]

print('Train size:', X_train.shape[0], 'val size:', X_val.shape[0])


# Trainning loop
n_epochs = 100
batch_size = 32
losses = []
val_losses = []
for epoch in range(n_epochs):
    model.train()
    perm = torch.randperm(X_train.size(0))
    epoch_loss = 0.0
    for i in range(0, X_train.size(0), batch_size):
        batch_idx = perm[i:i+batch_size]
        xb = X_train[batch_idx]
        yb = y_train[batch_idx]
        logits = model(xb)
        loss = loss_fn(logits, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item() * xb.size(0)
    epoch_loss /= X_train.size(0)
    losses.append(epoch_loss)
    # Validation
    model.eval()
    with torch.no_grad():
        v_logits = model(X_val)
        v_loss = loss_fn(v_logits, y_val).item()
        val_losses.append(v_loss)
    if (epoch+1) % 10 == 0:
        print(f'Epoch {epoch+1}/{n_epochs} - train loss: {epoch_loss:.4f} - val loss: {v_loss:.4f}')


# Plot losses
plt.figure()
plt.plot(losses, label='train')
plt.legend()
plt.title('Trainning and validation Loss')
plt.show()

# Evaluate accuracy on validation set
model.eval()
with torch.no_grad():
    preds = model(X_val).argmax(dim=1)
    acc = (preds == y_val).float().mean().item()
print('Validation accuracy:', acc)

# Save and Load Model
# Save model
torch.save(model.state_dict(), 'simple_mlp.pth')
print('Saved model to simple_mlp.pth')

# Load into new model instance
m2 = SimpleMLP()
m2.load_state_dict(torch.load('simple_mlp.pth'))
m2.eval()
print('Loaded model state into new instance')